{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5feoBnhZEebn/971aFnzV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Balacoumarane/finetune_llama/blob/main/FineTune_llama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlb6Y-G_1P1D"
      },
      "outputs": [],
      "source": [
        "# Fine-tune Llama 2 chat model on custom Q&A training dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Install library with dependencies"
      ],
      "metadata": {
        "id": "xiiirL3c2rhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 guardrail-ml==0.0.12 tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZSc4BLV21w8",
        "outputId": "5c3ea270-5d50-42a5-e208-c525174d75f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/244.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## load necessary libarries\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        "    set_seed,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftModel,\n",
        "    get_peft_model,\n",
        "    TaskType,\n",
        "    prepare_model_for_kbit_training)\n",
        "\n",
        "from peft.tuners.lora import LoraLayer\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# from guardrail.client import (\n",
        "#     run_metrics,\n",
        "#     run_simple_metrics,\n",
        "#     create_dataset)"
      ],
      "metadata": {
        "id": "5nBnT7mn212i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Fine tune (training model)"
      ],
      "metadata": {
        "id": "-w8l-W-nG_62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Define parameters for fine tuning"
      ],
      "metadata": {
        "id": "JZPhPzxMHEIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Used for multi-gpu\n",
        "local_rank = -1\n",
        "per_device_train_batch_size = 4\n",
        "per_device_eval_batch_size = 4\n",
        "gradient_accumulation_steps = 1\n",
        "learning_rate = 2e-4\n",
        "max_grad_norm = 0.3\n",
        "weight_decay = 0.001\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 64\n",
        "max_seq_length = None\n",
        "\n",
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"guardrail/llama-2-7b-guanaco-instruct-sharded\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"llama-2-7b-guanaco-dolly-mini\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"databricks/databricks-dolly-15k\"\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Activate nested quantization for 4-bit base models\n",
        "use_nested_quant = False\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 2\n",
        "\n",
        "# Enable fp16 training, (bf16 to True with an A100)\n",
        "fp16 = False\n",
        "\n",
        "# Enable bf16 training\n",
        "bf16 = False\n",
        "\n",
        "# Use packing dataset creating\n",
        "packing = False\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Optimizer to use, original is paged_adamw_32bit\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule (constant a bit better than cosine, and has advantage for analysis)\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of optimizer update steps, 10K original, 20 for demo purposes\n",
        "max_steps = -1\n",
        "\n",
        "# Fraction of steps to do a warmup for\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length (saves memory and speeds up training considerably)\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 10\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 1\n",
        "\n",
        "# The output directory where the model predictions and checkpoints will be written\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "# Visualize training\n",
        "report_to = \"tensorboard\"\n",
        "\n",
        "# Tensorboard logs\n",
        "tb_log_dir = \"./results/logs\""
      ],
      "metadata": {
        "id": "Zy-CfJ8l216I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Load custom dataset from tensorflow hub or local"
      ],
      "metadata": {
        "id": "wL--JUaMHJEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply prompt template per sample\n",
        "## !huggingface-cli login # uncomment to login to huggingface\n",
        "dataset = load_dataset(\"Bala2223/finetune_llm\", split=\"train\")\n",
        "\n",
        "# Shuffle the dataset\n",
        "dataset_shuffled = dataset.shuffle(seed=42)\n",
        "\n",
        "## split into train and test if required\n",
        "# dataset_shuffled = dataset_shuffled.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "cSdtU2hV22ER"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_shuffled[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qeM1EDw22Gq",
        "outputId": "d67a8140-00dd-4e92-e3cc-644b053d7922"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': \"Can Lamini generate text that is aligned with a given target language's grammar, syntax, or linguistic rules?\",\n",
              " 'answer': \"Yes, Lamini has the capability to generate text that aligns with a given target language's grammar, syntax, and linguistic rules. This is achieved through the use of language models that are trained on large datasets of text in the target language, allowing Lamini to generate text that is fluent and natural-sounding. Additionally, Lamini can be fine-tuned on specific domains or styles of language to further improve its ability to generate text that aligns with a given target language's linguistic rules.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Load model into GPU memory"
      ],
      "metadata": {
        "id": "L7sBAkK1Jy9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Load model into GPU memory"
      ],
      "metadata": {
        "id": "VhdI7glmJ1P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COPIED FROM https://github.com/artidoro/qlora/blob/main/qlora.py\n",
        "def print_trainable_parameters(model, use_4bit=False):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        num_params = param.numel()\n",
        "        # if using DS Zero 3 and the weights are initialized empty\n",
        "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
        "            num_params = param.ds_numel\n",
        "\n",
        "        all_param += num_params\n",
        "        if param.requires_grad:\n",
        "            trainable_params += num_params\n",
        "    if use_4bit:\n",
        "        trainable_params /= 2\n",
        "    print(\n",
        "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "6s254hnXBz1P"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COPIED FROM https://github.com/artidoro/qlora/blob/main/qlora.py\n",
        "def find_all_linear_names(model):\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, bnb.nn.Linear4bit):\n",
        "            names = name.split(\".\")\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
        "        lora_module_names.remove(\"lm_head\")\n",
        "    return list(lora_module_names)"
      ],
      "metadata": {
        "id": "N0QmXiF3B58z"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name):\n",
        "    # Load tokenizer and model with QLoRA configuration\n",
        "    compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=use_4bit,\n",
        "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=use_nested_quant,\n",
        "    )\n",
        "\n",
        "    if compute_dtype == torch.float16 and use_4bit:\n",
        "        major, _ = torch.cuda.get_device_capability()\n",
        "        if major >= 8:\n",
        "            print(\"=\" * 80)\n",
        "            print(\"Your GPU supports bfloat16, you can accelerate training with the argument --bf16\")\n",
        "            print(\"=\" * 80)\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        use_cache=False,\n",
        "        device_map=device_map,\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "\n",
        "    model.gradient_checkpointing_enable()\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    model.config.pretraining_tp = 1\n",
        "\n",
        "    #If only targeting attention blocks of the model\n",
        "    # target_modules = [\"q_proj\", \"v_proj\"]\n",
        "\n",
        "    #If targeting all linear layers\n",
        "    target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head']\n",
        "\n",
        "    # Load LoRA configuration\n",
        "    peft_config = LoraConfig(\n",
        "        lora_alpha=lora_alpha,\n",
        "        lora_dropout=lora_dropout,\n",
        "        target_modules = target_modules,\n",
        "        r=lora_r,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.CAUSAL_LM,,\n",
        "    )\n",
        "\n",
        "    # Load Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    return model, tokenizer, peft_config"
      ],
      "metadata": {
        "id": "NxFVNIrf219o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QWvw-r1221_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0j7M_7Sl6AjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Check with few prompts on the existing models"
      ],
      "metadata": {
        "id": "q2b1exODJ7cO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"how long does an American football match REALLY last, if you substract all the downtime?\"\n",
        "\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "j-V7Srn36Alr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"how long does an American football match REALLY last, if you substract all the downtime?\"\n",
        "\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "l4xf5Tqo6Anw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GXNQJp-H6App"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DZ39Pr7O6At5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 Start Fine-tuning"
      ],
      "metadata": {
        "id": "1J4IvqEEJ-rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.model.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "RuaHh26C6P8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x8Q0xpjh6P-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Restart runtime to clear VRAM\n",
        "1. runtime -> Restart runetime\n",
        "2. Run first three cells at top\n",
        "3. run the below"
      ],
      "metadata": {
        "id": "k9ZCxYLMKCDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6 Reload model and merge it with LORA weights"
      ],
      "metadata": {
        "id": "5DdG_wTsKEJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, output_dir)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "GLtaGJdZ6QH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7. Push models to hugging face hub"
      ],
      "metadata": {
        "id": "ncC8w3EJKHOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !huggingface-cli login\n",
        "\n",
        "model.push_to_hub(new_model, max_shard_size='2GB')\n",
        "tokenizer.push_to_hub(new_model)"
      ],
      "metadata": {
        "id": "Bw7zb3Sn7HEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restart runtime to clear VRAM to load in 4bit for inference\n",
        "1. runtime -> Restart runetime\n",
        "2. Run first **4x** cells at top\n",
        "3. run the below for inference"
      ],
      "metadata": {
        "id": "1tPLx3GDKKZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load new model for inference"
      ],
      "metadata": {
        "id": "UFm4gPSOKO1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface_profile = \"guardrail\"\n",
        "full_path = huggingface_profile + \"/\" + new_model\n",
        "\n",
        "model, tokenizer = load_model(full_path)"
      ],
      "metadata": {
        "id": "fSX1L1EH7Tpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Custom wrapper for response genarted by fine-tuned model"
      ],
      "metadata": {
        "id": "5K39BCjfKRmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_gen_eval_wrapper(model, tokenizer, prompt, model_id=1, show_metrics=True, temp=0.7, max_length=200):\n",
        "    \"\"\"\n",
        "    A wrapper function for inferencing, evaluating, and logging text generation pipeline.\n",
        "\n",
        "    Parameters:\n",
        "        model (str or object): The model name or the initialized text generation model.\n",
        "        tokenizer (str or object): The tokenizer name or the initialized tokenizer for the model.\n",
        "        prompt (str): The input prompt text for text generation.\n",
        "        model_id (int, optional): An identifier for the model. Defaults to 1.\n",
        "        show_metrics (bool, optional): Whether to calculate and show evaluation metrics.\n",
        "                                       Defaults to True.\n",
        "        max_length (int, optional): The maximum length of the generated text sequence.\n",
        "                                    Defaults to 200.\n",
        "\n",
        "    Returns:\n",
        "        generated_text (str): The generated text by the model.\n",
        "        metrics (dict): Evaluation metrics for the generated text (if show_metrics is True).\n",
        "    \"\"\"\n",
        "    # Suppress Hugging Face pipeline logging\n",
        "    logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "    # Initialize the pipeline\n",
        "    pipe = pipeline(task=\"text-generation\",\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_length=max_length,\n",
        "                    do_sample=True,\n",
        "                    temperature=temp)\n",
        "\n",
        "    # Generate text using the pipeline\n",
        "    pipe = pipeline(task=\"text-generation\",\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_length=200)\n",
        "    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Find the index of \"### Assistant\" in the generated text\n",
        "    index = generated_text.find(\"[/INST] \")\n",
        "    if index != -1:\n",
        "        # Extract the substring after \"### Assistant\"\n",
        "        substring_after_assistant = generated_text[index + len(\"[/INST] \"):].strip()\n",
        "    else:\n",
        "        # If \"### Assistant\" is not found, use the entire generated text\n",
        "        substring_after_assistant = generated_text.strip()\n",
        "\n",
        "    if show_metrics:\n",
        "        # Calculate evaluation metrics\n",
        "        metrics = run_metrics(substring_after_assistant, prompt, model_id)\n",
        "\n",
        "        return substring_after_assistant, metrics\n",
        "    else:\n",
        "        return substring_after_assistant\n"
      ],
      "metadata": {
        "id": "vYJa57S37TsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Few prompt to validate the fine-tuned model"
      ],
      "metadata": {
        "id": "dEAx8PhXKUTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference and evaluate outputs/prompts\n",
        "prompt = \"### Human: Sophie's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\"\n",
        "text_gen_eval_wrapper(model, tokenizer, prompt, show_metrics=False)"
      ],
      "metadata": {
        "id": "dg5w3S4H9hwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"### Human: From the passage provided, extract the names of the writers for the movie Captain America: The First Avenger. Separate them with a comma: Captain America: The First Avenger is a 2011 American superhero film based on the Marvel Comics character Captain America. Produced by Marvel Studios and distributed by Paramount Pictures, it is the fifth film in the Marvel Cinematic Universe (MCU). The film was directed by Joe Johnston, written by Christopher Markus and Stephen McFeely, and stars Chris Evans as Steve Rogers / Captain America alongside Tommy Lee Jones, Hugo Weaving, Hayley Atwell, Sebastian Stan, Dominic Cooper, Toby Jones, Neal McDonough, Derek Luke, and Stanley Tucci. During World War II, Steve Rogers, a frail man, is transformed into the super-soldier Captain America and must stop the Red Skull (Weaving) from using the Tesseract as an energy source for world domination. ### Assistant:\"\n",
        "generated_text = text_gen_eval_wrapper(model, tokenizer, prompt, show_metrics=False, max_length=300)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "U0SSZvQc9hyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ab9eGDg19h04"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}